import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances, silhouette_score
from scipy.stats import spearmanr, kendalltau, pearsonr
import matplotlib.pyplot as plt
from numpy.polynomial import Polynomial

# compute_learning_ease_with_task_transfer
# â”‚
# â”œâ”€ Step 0: åŸºæœ¬å‡†å¤‡ï¼ˆtask_ids, æ¯”ä¾‹ piï¼‰
# â”‚
# â”œâ”€ Step 1: å¯¹æ¯ä¸ª task å•ç‹¬ç®—ä¸€ä¸ª L_tï¼ˆtask å†…éƒ¨å¯å­¦ä¹ æ€§ï¼‰
# â”‚   â”œâ”€ task å†…æ ·æœ¬ç›¸ä¼¼åº¦ S_t
# â”‚   â”œâ”€ å±€éƒ¨ entropyï¼ˆh_localï¼‰
# â”‚   â”œâ”€ è¡¨å¾å¤æ‚åº¦ R_t
# â”‚   â”œâ”€ å¯†åº¦ / æœ‰æ•ˆè¦†ç›–åº¦ E_t
# â”‚   â””â”€ å¾—åˆ° raw L_t
# â”‚
# â”œâ”€ Step 2: ä»»åŠ¡ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼ˆtask â†’ task è¿ç§»ï¼‰
# â”‚   â”œâ”€ task center
# â”‚   â”œâ”€ task similarity matrix
# â”‚   â””â”€ è·¨ä»»åŠ¡åŠ æƒ L_t_adj
# â”‚
# â””â”€ è¾“å‡ºï¼š
#     â”œâ”€ L_dataset
#     â””â”€ æ¯ä¸ª task çš„ L_t_adj


def covariance_entropy(X):
    """
    X: (N_t, D)
    è¿”å›å½’ä¸€åŒ–åæ–¹å·®ç†µ
    """
    if X.shape[0] <= 1:
        return 0.0
    X_centered = X - X.mean(axis=0)
    cov = np.cov(X_centered, rowvar=False)  # (D, D)
    eigvals = np.linalg.eigvalsh(cov)
    eigvals = np.maximum(eigvals, 1e-12)
    p = eigvals / eigvals.sum()
    H = -np.sum(p * np.log(p))
    # å½’ä¸€åŒ–åˆ° [0,1]
    # H_norm = H / np.log(len(p))
    H_norm = H
    return H_norm

def compute_learning_ease_with_task_transfer(task_groups, beta=0.8, sigma=None):
    """
    Task-centric version of compute_learning_ease_with_task_transfer.
    
    Args:
        task_groups: list of dicts, each dict has
            {
                "task_id": int,
                "features": (Nt, D) np.array,
                "demo_lengths": (Nt,) np.array,
                "task_length": float,
                "task_description": str
            }
        beta: trade-off coefficient
        sigma: kernel bandwidth (float or None)
    
    Returns:
        dataset_score: float
        task_scores: dict, task_id -> float
    """

    task_ids = [g["task_id"] for g in task_groups]
    n_tasks = len(task_groups)

    # Step 1: è®¡ç®—æ¯ä¸ª task çš„ L_t å’Œ task center
    L_t_raw = {}
    task_centers = {}

    for i, g in enumerate(task_groups):
        X_t = g["features"]
        N_t = X_t.shape[0]
        if N_t <= 1:
            L_t_raw[g["task_id"]] = 0.0
            task_centers[g["task_id"]] = X_t.mean(axis=0) if N_t>0 else np.zeros(X_t.shape[1])
            continue

        # pairwise squared distances
        dists_t = pairwise_distances(X_t, X_t, metric="euclidean") ** 2
        sigma_t = 0.001 if sigma is None else sigma
        S_t = np.exp(-dists_t / (2 * sigma_t**2))
        P_t = S_t / S_t.sum(axis=1, keepdims=True)

        h_local = -np.sum(P_t * np.log(P_t + 1e-12), axis=1).mean()
        d_avg = np.mean(np.sqrt(dists_t[np.triu_indices(N_t, 1)]))

        # R_t å¯ä»¥æŒ‰åŸæ¥çš„ covariance_entropy
        R_t = covariance_entropy(X_t) * np.tanh(d_avg / sigma_t)

        rho_t = S_t.mean(axis=1)
        E_t = rho_t.mean()
        E_t = E_t / np.log10(1 + g["task_length"])

        # L_t = (R_t**beta) * (E_t**(1-beta))
        L_t_raw[g["task_id"]] = (R_t**beta) * (E_t**(1-beta))
        print(L_t_raw)

        # task center
        task_centers[g["task_id"]] = X_t.mean(axis=0)

    # Step 2: è®¡ç®— task â†’ task ç›¸ä¼¼åº¦
    centers_array = np.stack([task_centers[t] for t in task_ids])
    center_dists = pairwise_distances(centers_array, centers_array, metric="euclidean") ** 2
    sigma_center = np.median(np.sqrt(center_dists)) if sigma is None else sigma
    S_task = np.exp(-center_dists / (2 * sigma_center**2))

    # Step 3: è·¨ä»»åŠ¡åŠ æƒ L_t_adj
    # è¿™é‡Œå¯ä»¥ç”¨ pi æƒé‡æˆ–å‡åŒ€æƒé‡
    task_scores = {}
    pi_first = 0.1
    pi_second = 0.1
    split_idx = n_tasks // 2
    pi_scale = 0.01698373

    for i, t in enumerate(task_ids):
        pi_t = pi_first if i < split_idx else pi_second
        pi_t = np.tanh(pi_t / pi_scale)

        L_t_adj = sum(S_task[i, j] * L_t_raw[task_ids[j]] for j in range(n_tasks))
        task_scores[t] = L_t_adj * pi_t

    # Step 4: dataset-level leanability
    dataset_score = np.mean(list(task_scores.values()))

    return dataset_score, task_scores




# def compute_task_diversity_entropy(X, sigma=0.1, eps=1e-12):
#     """
#     Kernel-based entropy estimator:
#       H_hat = - (1/N) * sum_i log( (1/N) * sum_j K_sigma(x_i, x_j) )
#     """
#     X = np.asarray(X)
#     N = X.shape[0]

#     # pairwise euclidean distances
#     dists = pairwise_distances(X, X, metric="euclidean")

#     # choose sigma if not provided: median of upper triangular distances
#     if sigma is None:
#         iu = np.triu_indices(N, k=1)
#         if iu[0].size > 0:
#             sigma = np.median(dists[iu])
#             if sigma == 0:
#                 nonzero = dists[iu][dists[iu] > 0]
#                 sigma = np.median(nonzero) if nonzero.size > 0 else 1.0
#         else:
#             sigma = 1.0

#     K = np.exp(-(dists**2) / (2.0 * (sigma ** 2)))
#     inner = K.mean(axis=1)
#     H_hat = - np.mean(np.log(inner + eps))
#     return H_hat, sigma

# def subsample_dataset(X, y, dataset_name):
#     """æŒ‰ç…§æ¯”ä¾‹éšæœºä¸‹é‡‡æ ·ï¼Œæ¯ä¸ªä»»åŠ¡ä¿ç•™ä¸€å®šæ¯”ä¾‹çš„æ ·æœ¬"""
#     if dataset_name not in dataset_ratios:
#         return X, y

#     ratios = dataset_ratios[dataset_name]
#     unique_tasks = np.unique(y)

#     new_X_list, new_y_list = [], []
#     rng = np.random.default_rng(42)

#     n_tasks = len(unique_tasks)
#     split_idx = n_tasks // 2  # å‰ä¸€åŠç”¨ firstï¼Œåä¸€åŠç”¨ second

#     for i, t in enumerate(unique_tasks):
#         X_t = X[y == t]
#         if i < split_idx:
#             ratio = ratios["first"]
#         else:
#             ratio = ratios["second"]

#         sample_size = int(np.round(len(X_t) * ratio))
#         sample_size = max(sample_size, 1)  # è‡³å°‘ä¿ç•™ä¸€ä¸ªæ ·æœ¬

#         idx = rng.choice(len(X_t), size=sample_size, replace=False)
#         new_X_list.append(X_t[idx])
#         new_y_list.append(np.full(sample_size, t))

#     return np.vstack(new_X_list), np.concatenate(new_y_list)

# def covariance_entropy(X):
#     """
#     X: (N_t, D)
#     è¿”å›å½’ä¸€åŒ–åæ–¹å·®ç†µ
#     """
#     if X.shape[0] <= 1:
#         return 0.0
#     X_centered = X - X.mean(axis=0)
#     cov = np.cov(X_centered, rowvar=False)  # (D, D)
#     eigvals = np.linalg.eigvalsh(cov)
#     eigvals = np.maximum(eigvals, 1e-12)
#     p = eigvals / eigvals.sum()
#     H = -np.sum(p * np.log(p))
#     # å½’ä¸€åŒ–åˆ° [0,1]
#     # H_norm = H / np.log(len(p))
#     H_norm = H
#     return H_norm

# def compute_learning_ease_with_task_transfer(X, y, task_lengths, dataset_name, beta=0.8, pi_scale=0.01698373, sigma=None):
#     """
#     Compute Learning Ease for a dataset with task-level transfer consideration.
    
#     Args:
#         X: np.array, shape (N, D), features
#         y: np.array, shape (N,), task ids
#         task_lengths: dict, æ¯ä¸ªä»»åŠ¡çš„é•¿åº¦
#         dataset_name: str, å½“å‰æ•°æ®é›†åå­—ï¼Œç”¨äºæŸ¥ pi æ¯”ä¾‹
#         beta: float, trade-off between robustness and overfitting ease
#         sigma: float or None, kernel bandwidth for similarity (if None, median distance is used)
#     """
#     task_ids = np.unique(y)
#     N = X.shape[0]

#     L_t_raw = {}
#     task_centers = {}

#     # å–å½“å‰æ•°æ®é›†å¯¹åº”çš„æ¯”ä¾‹
#     ratios = dataset_ratios.get(dataset_name, {"first": 0.1, "second": 0.1})
#     n_tasks = len(task_ids)
#     split_idx = n_tasks // 2

#     # Step 1: compute raw L_t per task
#     for i, t in enumerate(task_ids):
#         X_t = X[y == t]
#         N_t = X_t.shape[0]
#         if N_t <= 1:
#             continue
        
#         # pairwise squared distances
#         dists_t = pairwise_distances(X_t, X_t, metric="euclidean") ** 2
#         sigma_t = 0.001 if sigma is None else sigma
#         S_t = np.exp(-dists_t / (2 * sigma_t**2))
#         P_t = S_t / S_t.sum(axis=1, keepdims=True)

#         h_local = -np.sum(P_t * np.log(P_t + 1e-12), axis=1).mean()
#         d_avg = np.mean(np.sqrt(dists_t[np.triu_indices(N_t, 1)]))

#         R_t = covariance_entropy(X_t) * np.tanh(d_avg / sigma_t)

#         rho_t = S_t.mean(axis=1)
#         E_t = rho_t.mean()
#         if t in task_lengths:
#             E_t = E_t / np.log10(1 + task_lengths[t])

#         # ğŸ”‘ ç”¨å›ºå®šæ¯”ä¾‹æ›¿ä»£ N_t/N
#         pi_t = ratios["first"] if i < split_idx else ratios["second"]
#         pi_scale = 0.01698373 #0.02016270
#         pi_t = np.tanh(pi_t / pi_scale)
#         L_t = (R_t**beta) * (E_t**(1 - beta))
#         L_t_raw[t] = L_t
#         task_centers[t] = X_t.mean(axis=0)

#     # Step 2: task similarity
#     centers_array = np.stack([task_centers[t] for t in task_ids])
#     center_dists = pairwise_distances(centers_array, centers_array, metric="euclidean") ** 2
#     sigma_center = np.median(np.sqrt(center_dists)) if sigma is None else 0.01
#     S_task = np.exp(-center_dists / (2 * sigma_center**2))

#     L_t_adjusted = {}
#     for i, t in enumerate(task_ids):
#         # print("sinma_center:", sigma_center)
#         L_t_adj = sum(S_task[i, j] * L_t_raw[task_ids[j]] for j in range(len(task_ids)))
#         L_t_adjusted[t] = L_t_adj*pi_t

#     L_dataset = np.mean(list(L_t_adjusted.values()))
#     return L_dataset, L_t_adjusted


# # ---------- (2) ç›¸å…³æ€§ ----------
# def compute_correlations(x, y):
#     srocc, _ = spearmanr(x, y)
#     krocc, _ = kendalltau(x, y)
#     plcc, _ = pearsonr(x, y)
#     return srocc, krocc, plcc

# # ---------- (3) ä¸»ç¨‹åº ----------
# # ---------- (4) ç»˜åˆ¶æ‹Ÿåˆå›¾ ----------
# import matplotlib.pyplot as plt
# import numpy as np

# def plot_fit_vs_gt_multi(y_trues, y_preds, labels, markers=None, colors=None, title=None, fontsize=24):
#     """
#     y_trues, y_preds: list of np.arraysï¼Œæ¯ä¸ªå…ƒç´ å¯¹åº”ä¸€ä¸ªæ•°æ®é›†
#     labels: list of strï¼Œæ•°æ®é›†åç§°
#     markers: list of strï¼Œç‚¹çš„æ ·å¼
#     colors: list of strï¼Œç‚¹çš„é¢œè‰²
#     fontsize: intï¼Œæ•´ä½“å­—ä½“å¤§å°
#     """
#     import matplotlib.pyplot as plt
#     import numpy as np

#     if markers is None:
#         markers = ['o', 's', 'D', '^', 'v', '*', 'x', '+', 'p', 'H', '8', '<', '>', '|', '_', '.', ','] * 3

#     if colors is None:
#         base_colors = plt.get_cmap('tab10').colors
#         colors = [base_colors[i % len(base_colors)] for i in range(len(labels))]

#     # åˆå¹¶æ‰€æœ‰æ•°æ®ç”¨äºå…¨å±€å½’ä¸€åŒ–
#     all_y_true = np.concatenate(y_trues)
#     all_y_pred = np.concatenate(y_preds)

#     # å…¨å±€å½’ä¸€åŒ–åˆ° [20, 100] ç”¨äºç»˜å›¾
#     y_min, y_max = all_y_pred.min(), all_y_pred.max()
#     all_y_pred_scaled = 20 + (all_y_pred - y_min) / (y_max - y_min) * (100 - 20)

#     # ä¸‰æ¬¡å¤šé¡¹å¼æ‹Ÿåˆ
#     coefs = np.polyfit(all_y_true, all_y_pred_scaled, 3)
#     poly = np.poly1d(coefs)
#     x_fit = np.linspace(min(all_y_true), max(all_y_true), 200)
#     y_fit = poly(x_fit)

#     plt.figure(figsize=(10,4))
#     start_idx = 0
#     dataset_means = []

#     for i in range(len(y_trues)):
#         n = len(y_trues[i])
#         y_scaled = all_y_pred_scaled[start_idx:start_idx+n]

#         # æ•£ç‚¹
#         plt.scatter(y_trues[i], y_scaled,
#                     label=labels[i], marker=markers[i], color=colors[i],
#                     s=70, alpha=0.8, edgecolor='None')

#         # è®¡ç®—å¹³å‡å€¼
#         mean_pred = np.mean(y_scaled)
#         mean_gt = np.mean(y_trues[i])
#         dataset_means.append((mean_pred, mean_gt))

#         # ç»˜åˆ¶æ°´å¹³è™šçº¿ï¼ˆé¢„æµ‹å¹³å‡å€¼ï¼‰
#         plt.hlines(mean_pred, xmin=min(y_trues[i]), xmax=max(y_trues[i]),
#                    colors=colors[i], linestyles='dashed', linewidth=1.3)
#         # ç»˜åˆ¶å‚ç›´è™šçº¿ï¼ˆGTå¹³å‡å€¼ï¼‰
#         plt.vlines(mean_gt, ymin=min(y_scaled), ymax=max(y_scaled),
#                    colors=colors[i], linestyles='dashed', linewidth=1.3)

#         # æ ‡æ³¨é¢„æµ‹å‡å€¼ï¼Œåœ¨æ°´å¹³è™šçº¿æœ€å·¦ç«¯
#         plt.text(min(y_trues[i]), mean_pred, f"{mean_pred:.1f}", color=colors[i],
#                  fontsize=fontsize - 10, verticalalignment='bottom', horizontalalignment='left')

#         # æ ‡æ³¨GTå‡å€¼ï¼Œåœ¨ç«–çº¿æœ€ä¸‹ç«¯
#         plt.text(mean_gt, min(y_scaled), f"{mean_gt:.1f}", color=colors[i],
#                  fontsize=fontsize - 10, verticalalignment='bottom', horizontalalignment='left')

#         start_idx += n

#     # æ‹Ÿåˆæ›²çº¿
#     plt.plot(x_fit, y_fit, color='red', linestyle='--', linewidth=2, label='Cubic Fit')

#     # è®¾ç½®å­—ä½“å¤§å°
#     plt.xlabel("Ground Truth", fontsize=fontsize-4)
#     plt.ylabel("Predicted", fontsize=fontsize-4)
#     if title:
#         plt.title(title, fontsize=fontsize + 2)
#     plt.xticks(fontsize=fontsize - 8)
#     plt.yticks(fontsize=fontsize - 8)
#     plt.grid(True, linestyle='--', alpha=0.5)
#     plt.legend(fontsize=fontsize - 4, loc='lower right',bbox_to_anchor=(1.5, 0))

#     # ç›¸å…³æ€§æŒ‡æ ‡ç”¨åŸå§‹é¢„æµ‹å€¼è®¡ç®—
#     srocc, krocc, plcc = compute_correlations(all_y_true, all_y_pred)
#     plt.text(0.05, 0.95, f"SRCC={srocc:.3f}\nKRCC={krocc:.3f}\nPLCC={plcc:.3f}",
#              transform=plt.gca().transAxes, verticalalignment='top',
#              fontsize=fontsize - 9,
#              bbox=dict(facecolor='white', alpha=0.6, edgecolor='gray'))

#     plt.tight_layout()
#     plt.savefig("learning_ease_fit_multi.png", dpi=300)
#     plt.show()

#     return dataset_means  # è¿”å›æ¯ä¸ªæ•°æ®é›†çš„ (é¢„æµ‹å‡å€¼, GTå‡å€¼)

